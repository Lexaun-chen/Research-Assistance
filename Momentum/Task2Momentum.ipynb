{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d067493d662b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.command.install import install\n",
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "id": "68ad8e02-daeb-46d2-801c-6b7789728a90",
   "metadata": {},
   "source": [
    "! brew install pandoc  "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5e91b15-00d6-458a-8190-7a6d7af3c288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T12:40:24.492776Z",
     "start_time": "2024-10-21T12:40:22.681143Z"
    }
   },
   "source": [
    "################################################\n",
    "# Jegadeesh & Titman (1993) Momentum Portfolio #\n",
    "# December 2024                                #  \n",
    "# Lexuan Chen                                  #\n",
    "################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrds\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.offsets import *\n",
    "from scipy import stats"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "837479aa-46ed-4de3-b6c5-2066f79b126d",
   "metadata": {},
   "source": [
    "###################\n",
    "# Connect to WRDS #\n",
    "###################\n",
    "conn=wrds.Connection()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2c8cd-50d1-4191-9c52-008990d8f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "###################\n",
    "# CRSP Block      #\n",
    "###################\n",
    "# sql similar to crspmerge macro\n",
    "# added exchcd=-2,-1,0 to address the issue that stocks temp stopped trading\n",
    "# without exchcd=-2,-1, 0 the non-trading months will be tossed out in the output\n",
    "# leading to wrong cumret calculation in momentum step\n",
    "# Code\tDefinition\n",
    "# -2\tHalted by the NYSE or AMEX\n",
    "# -1\tSuspended by the NYSE, AMEX, or NASDAQ\n",
    "# 0\tNot Trading on NYSE, AMEX, or NASDAQ\n",
    "# 1\tNew York Stock Exchange\n",
    "# 2\tAmerican Stock Exchange\n",
    "\n",
    "crsp_m = conn.raw_sql(\"\"\"\n",
    "                      select a.permno, a.date,\n",
    "                      b.shrcd, b.exchcd, \n",
    "                      a.ret, a.vol, a.shrout, a.prc\n",
    "                      from crsp.msf as a\n",
    "                      left join crsp.msenames as b\n",
    "                      on a.permno=b.permno\n",
    "                      and b.namedt<=a.date\n",
    "                      and a.date<=b.nameendt\n",
    "                      where a.date between '01/01/2000' and '12/31/2023'\n",
    "                      and b.exchcd between -2 and 2\n",
    "                      and b.shrcd between 10 and 11\n",
    "                      \"\"\") \n",
    "\n",
    "# Change variable format to int\n",
    "crsp_m[['permno','shrcd','exchcd']]=\\\n",
    "    crsp_m[['permno','shrcd','exchcd']].astype(int)\n",
    "\n",
    "# Line up date to be end of month\n",
    "crsp_m['date']=pd.to_datetime(crsp_m['date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ee05d-257a-4ebf-bfa2-697d0309be8b",
   "metadata": {},
   "source": [
    "## Returns of Relative Strength Portfolios\n",
    "In this part, we replicate Table 2 of Jegadeesh and Titman (1993). \n",
    "The relative strength portfolios are formed based on J-month lagged returns and held for K months.\n",
    "Since this is an univariate portfolio test, we perform decile sorts and portfolio returns are equal-weighted.\n",
    "Sample period is from Januar2000 to December 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee63b4a-87c8-4535-8890-0dcadb0f84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "J_list = [3,6,9,12]\n",
    "K_list = [3,6,9,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f82ea-989a-41b9-9b19-245c4d3c300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_crsp = crsp_m[['permno','date','ret']].sort_values(['permno','date'])\\\n",
    "    .set_index('date')\n",
    "_tmp_crsp['ret']=_tmp_crsp['ret'].fillna(0)\n",
    "_tmp_crsp['logret']=np.log(1+_tmp_crsp['ret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f87958-9ba1-44b1-8cbe-a65bf667fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_crsp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6a048-8c5f-4d20-8fac-ab2fc85204cd",
   "metadata": {},
   "source": [
    "### Panel A: No skip between portfolio formation and holding period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359607c7-4225-466b-83f4-c2d21dff266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "umd_series = {}\n",
    "tmp_umd_series = {}\n",
    "for J in J_list:\n",
    "    for K in K_list: \n",
    "        umd = _tmp_crsp.groupby('permno')['logret'].rolling(J, min_periods=J).sum().reset_index()  \n",
    "        umd['cumret']=np.exp(umd['logret'])-1\n",
    "    \n",
    "        ########################################\n",
    "        # Formation of 10 Momentum Portfolios  #\n",
    "        ########################################\n",
    "        umd=umd.dropna(axis=0, subset=['cumret'])\n",
    "        umd['momr']=umd.groupby('date')['cumret'].transform(lambda x: pd.qcut(x, 10, labels=False))\n",
    "        umd['momr'] = umd['momr'].astype(int) \n",
    "        umd['momr'] = umd['momr']+1\n",
    "    \n",
    "        umd['form_date'] = umd['date']\n",
    "        umd['medate'] = umd['date']+MonthEnd(0)\n",
    "        umd['hdate1']=umd['medate']+MonthBegin(1) \n",
    "        umd['hdate2']=umd['medate']+MonthEnd(K)   ###是这里不一样！！！\n",
    "\n",
    "        tmp1 = umd\n",
    "        tmp1['hdate1_m'] = pd.DatetimeIndex(tmp1['hdate1']).month\n",
    "        tmp1['hdate2_m'] = pd.DatetimeIndex(tmp1['hdate2']).month\n",
    "        \n",
    "        #Calculate the monthly difference (mgap) of the holding period and deal with cross-year situations\n",
    "        tmp1['mgap'] = tmp1.hdate2_m - tmp1.hdate1_m\n",
    "        tmp1['mgap'] = np.where(tmp1.mgap>0, tmp1.mgap, tmp1.mgap+12)\n",
    "        tmp_umd_series[(J,K)] = tmp1\n",
    "\n",
    "        umd = umd[['permno','form_date','momr','hdate1','hdate2']]\n",
    "        umd_series[(J,K)] = umd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45a2a8-0205-40ff-bd9f-8173aeb7f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "Panel_A = {}\n",
    "_tmp_ret = crsp_m[['permno','date','ret']]\n",
    "_tmp_ret['ret'] = _tmp_ret['ret'].fillna(0) # 像这一行 其实前面最开始处理的时候你已经处理过了 你又搞一遍\n",
    "\n",
    "for J in J_list:\n",
    "    for K in K_list: \n",
    "        umd = umd_series[(K,J)]\n",
    "        \n",
    "        port = pd.DataFrame()\n",
    "        n = 100000\n",
    "        \n",
    "        for i in range(0, _tmp_ret.shape[0], n): \n",
    "            \n",
    "            chunks =   _tmp_ret.iloc[i:i + n]\n",
    "            merged = umd.merge(chunks, on=['permno'], how='inner')\n",
    "            merged = merged[(merged['hdate1']<=merged['date']) & (merged['date']<=merged['hdate2'])]\n",
    "            port = pd.concat([port, merged],ignore_index=True )\n",
    "        \n",
    "        umd2 = port.sort_values(by=['date','momr','form_date','permno']).drop_duplicates()\n",
    "        \n",
    "        umd3 = umd2.groupby(['date','momr','form_date'])['ret'].mean().reset_index()\n",
    "        start_yr = umd3['date'].dt.year.min()+2\n",
    "        umd3 = umd3[umd3['date'].dt.year>=start_yr]\n",
    "        umd3 = umd3.sort_values(by=['date','momr'])\n",
    "        \n",
    "        # Create one return series per MOM group every month\n",
    "        ewret = umd3.groupby(['date','momr'])['ret'].mean().reset_index()\n",
    "        ewstd = umd3.groupby(['date','momr'])['ret'].std().reset_index()\n",
    "        ewret = ewret.rename(columns={'ret':'ewret'})\n",
    "        ewstd = ewstd.rename(columns={'ret':'ewretstd'})\n",
    "        ewretdat = pd.merge(ewret, ewstd, on=['date','momr'], how='inner')\n",
    "        ewretdat = ewretdat.sort_values(by=['momr'])\n",
    "        \n",
    "        # Transpose portfolio layout to have columns as portfolio returns\n",
    "        ewretdat2 = ewretdat.pivot(index='date', columns='momr', values='ewret')\n",
    "        \n",
    "        # Add prefix port in front of each column\n",
    "        ewretdat2 = ewretdat2.add_prefix('port') \n",
    "        ewretdat2 = ewretdat2.rename(columns={'port1':'losers', 'port10':'winners'})\n",
    "        ewretdat2['long_short'] = ewretdat2['winners'] - ewretdat2['losers']\n",
    "        \n",
    "        # Compute Long-Short Portfolio Cumulative Returns\n",
    "        ewretdat3 = ewretdat2\n",
    "        ewretdat3['1+losers']=1+ewretdat3['losers']\n",
    "        ewretdat3['1+winners']=1+ewretdat3['winners']\n",
    "        ewretdat3['1+ls'] = 1+ewretdat3['long_short']\n",
    "        \n",
    "        #cumprod(): Calculate the cumulative product for each sequence,\n",
    "        ewretdat3['cumret_winners']=ewretdat3['1+winners'].cumprod()-1\n",
    "        ewretdat3['cumret_losers']=ewretdat3['1+losers'].cumprod()-1\n",
    "        ewretdat3['cumret_long_short']=ewretdat3['1+ls'].cumprod()-1\n",
    "        \n",
    "        #################################\n",
    "        # Portfolio Summary Statistics  #\n",
    "        ################################# \n",
    "        \n",
    "        # Mean \n",
    "        mom_mean = ewretdat3[['winners', 'losers', 'long_short']].mean().to_frame()\n",
    "        mom_mean = mom_mean.rename(columns={0:f'{K}_mean'}).reset_index()\n",
    "        \n",
    "        # T-Value and P-Value\n",
    "        t_losers = pd.Series(stats.ttest_1samp(ewretdat3['losers'],0.0)).to_frame().T\n",
    "        t_winners = pd.Series(stats.ttest_1samp(ewretdat3['winners'],0.0)).to_frame().T\n",
    "        t_long_short = pd.Series(stats.ttest_1samp(ewretdat3['long_short'],0.0)).to_frame().T\n",
    "        \n",
    "        t_losers['momr']='losers'\n",
    "        t_winners['momr']='winners'\n",
    "        t_long_short['momr']='long_short'\n",
    "        \n",
    "        t_output =pd.concat([t_winners, t_losers, t_long_short])\\\n",
    "            .rename(columns={0:f'{K}_t-stat', 1:f'{K}_p-value'})\n",
    "        \n",
    "        # Combine mean, t and p\n",
    "        mom_output = pd.merge(mom_mean, t_output, on=['momr'], how='inner')\n",
    "        mom_output['momr'] = [f'{J} Sell', f'{J} Buy', f'{J} Buy-sell']\n",
    "        \n",
    "        mom_output = mom_output[['momr',f'{K}_mean',f'{K}_t-stat']]\n",
    "        Panel_A[(J,K)] = mom_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716c9ef-411b-4fd0-b5f7-7fe9a93a22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = pd.concat([Panel_A[(3,3)],Panel_A[(6,3)],Panel_A[(9,3)],Panel_A[(12,3)]],ignore_index=True)\n",
    "results2 = pd.concat([Panel_A[(3,6)],Panel_A[(6,6)],Panel_A[(9,6)],Panel_A[(12,6)]],ignore_index=True)     \n",
    "results3 = pd.concat([Panel_A[(3,9)],Panel_A[(6,9)],Panel_A[(9,9)],Panel_A[(12,9)]],ignore_index=True) \n",
    "results4 = pd.concat([Panel_A[(3,12)],Panel_A[(6,12)],Panel_A[(9,12)],Panel_A[(12,12)]],ignore_index=True) \n",
    "\n",
    "result = pd.merge(results1, results2, on=['momr'], how='inner')\n",
    "result = pd.merge(result, results3, on=['momr'], how='inner')\n",
    "result = pd.merge(result, results4, on=['momr'], how='inner')\n",
    "result.rename(columns={'momr':'J'},inplace=True)\n",
    "result = result[['J','3_mean','3_t-stat','6_mean','6_t-stat','9_mean','9_t-stat','12_mean','12_t-stat']].round(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b149e7-ac88-4f48-8c12-2d125497615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('Table 2 Panel_A.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb17c1e-8d83-4a51-93be-27f0b8d1151a",
   "metadata": {},
   "source": [
    "### Panel B: One-month skip between portfolio formation and holding period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d280f6b-fe44-492c-9194-3de9903ea0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_umd_series = {}\n",
    "skip_tmp_umd_series = {}\n",
    "for J in J_list:\n",
    "    for K in K_list: \n",
    "        '''\n",
    "        umd = _tmp_crsp.groupby('permno')['logret'].rolling(J, min_periods=J-1).sum().reset_index()  \n",
    "        这里出错了老铁！！！！！！\n",
    "        '''\n",
    "        ###应该是\n",
    "        umd = _tmp_crsp.groupby('permno')['logret'].rolling(J-1, min_periods=J-1).sum().reset_index()\n",
    "\n",
    "        umd['cumret']=np.exp(umd['logret'])-1\n",
    "    \n",
    "        ########################################\n",
    "        # Formation of 10 Momentum Portfolios  #\n",
    "        ########################################\n",
    "        umd=umd.dropna(axis=0, subset=['cumret'])\n",
    "        umd['momr']=umd.groupby('date')['cumret'].transform(lambda x: pd.qcut(x, 10, labels=False))\n",
    "        umd['momr'] = umd['momr'].astype(int) \n",
    "        umd['momr'] = umd['momr']+1\n",
    "    \n",
    "        umd['form_date'] = umd['date']\n",
    "        umd['medate'] = umd['date']+MonthEnd(0)\n",
    "        umd['hdate1']=umd['medate']+MonthBegin(1) \n",
    "        umd['hdate2']=umd['medate']+MonthEnd(K)\n",
    "\n",
    "        tmp1 = umd\n",
    "        tmp1['hdate1_m'] = pd.DatetimeIndex(tmp1['hdate1']).month\n",
    "        tmp1['hdate2_m'] = pd.DatetimeIndex(tmp1['hdate2']).month\n",
    "        \n",
    "        #Calculate the monthly difference (mgap) of the holding period and deal with cross-year situations\n",
    "        tmp1['mgap'] = tmp1.hdate2_m - tmp1.hdate1_m\n",
    "        tmp1['mgap'] = np.where(tmp1.mgap>0, tmp1.mgap, tmp1.mgap+12)\n",
    "        skip_tmp_umd_series[(J,K)] = tmp1\n",
    "\n",
    "        umd = umd[['permno','form_date','momr','hdate1','hdate2']]\n",
    "        skip_umd_series[(J,K)] = umd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f22f3d-90ef-4562-96a3-c989ae657f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Panel_B = {}\n",
    "_tmp_ret = crsp_m[['permno','date','ret']]\n",
    "_tmp_ret['ret'] = _tmp_ret['ret'].fillna(0)\n",
    "\n",
    "for J in J_list:\n",
    "    for K in K_list: \n",
    "        umd = umd_series[(K,J)]\n",
    "        \n",
    "        port = pd.DataFrame()\n",
    "        n = 100000\n",
    "        \n",
    "        for i in range(0, _tmp_ret.shape[0], n): \n",
    "            \n",
    "            chunks =   _tmp_ret.iloc[i:i + n]\n",
    "            merged = umd.merge(chunks, on=['permno'], how='inner')\n",
    "            merged = merged[(merged['hdate1']<=merged['date']) & (merged['date']<=merged['hdate2'])]\n",
    "            port = pd.concat([port, merged],ignore_index=True )\n",
    "        \n",
    "        umd2 = port.sort_values(by=['date','momr','form_date','permno']).drop_duplicates()\n",
    "        \n",
    "        umd3 = umd2.groupby(['date','momr','form_date'])['ret'].mean().reset_index()\n",
    "        start_yr = umd3['date'].dt.year.min()+2\n",
    "        #获取数据集中的最小年份，并加上2，表示从 形成期结束后的第二年 开始计算。\n",
    "        # 这样做是为了避免在形成期初期的数据不够稳定时使用数据。\n",
    "        \n",
    "        umd3 = umd3[umd3['date'].dt.year>=start_yr]\n",
    "        umd3 = umd3.sort_values(by=['date','momr'])\n",
    "        \n",
    "        # Create one return series per MOM group every month\n",
    "        ewret = umd3.groupby(['date','momr'])['ret'].mean().reset_index()\n",
    "        ewstd = umd3.groupby(['date','momr'])['ret'].std().reset_index()\n",
    "        ewret = ewret.rename(columns={'ret':'ewret'})\n",
    "        ewstd = ewstd.rename(columns={'ret':'ewretstd'})\n",
    "        ewretdat = pd.merge(ewret, ewstd, on=['date','momr'], how='inner')\n",
    "        ewretdat = ewretdat.sort_values(by=['momr'])\n",
    "        \n",
    "        # Transpose portfolio layout to have columns as portfolio returns\n",
    "        ewretdat2 = ewretdat.pivot(index='date', columns='momr', values='ewret')\n",
    "        \n",
    "        # Add prefix port in front of each column\n",
    "        ewretdat2 = ewretdat2.add_prefix('port') \n",
    "        ewretdat2 = ewretdat2.rename(columns={'port1':'losers', 'port10':'winners'})\n",
    "        ewretdat2['long_short'] = ewretdat2['winners'] - ewretdat2['losers']\n",
    "        \n",
    "        # Compute Long-Short Portfolio Cumulative Returns\n",
    "        ewretdat3 = ewretdat2\n",
    "        ewretdat3['1+losers']=1+ewretdat3['losers']\n",
    "        ewretdat3['1+winners']=1+ewretdat3['winners']\n",
    "        ewretdat3['1+ls'] = 1+ewretdat3['long_short']\n",
    "        \n",
    "        #cumprod(): Calculate the cumulative product for each sequence,\n",
    "        ewretdat3['cumret_winners']=ewretdat3['1+winners'].cumprod()-1\n",
    "        ewretdat3['cumret_losers']=ewretdat3['1+losers'].cumprod()-1\n",
    "        ewretdat3['cumret_long_short']=ewretdat3['1+ls'].cumprod()-1\n",
    "        \n",
    "        #################################\n",
    "        # Portfolio Summary Statistics  #\n",
    "        ################################# \n",
    "        \n",
    "        # Mean \n",
    "        mom_mean = ewretdat3[['winners', 'losers', 'long_short']].mean().to_frame()\n",
    "        mom_mean = mom_mean.rename(columns={0:f'{K}_mean'}).reset_index()\n",
    "        \n",
    "        # T-Value and P-Value\n",
    "        t_losers = pd.Series(stats.ttest_1samp(ewretdat3['losers'],0.0)).to_frame().T\n",
    "        t_winners = pd.Series(stats.ttest_1samp(ewretdat3['winners'],0.0)).to_frame().T\n",
    "        t_long_short = pd.Series(stats.ttest_1samp(ewretdat3['long_short'],0.0)).to_frame().T\n",
    "        \n",
    "        t_losers['momr']='losers'\n",
    "        t_winners['momr']='winners'\n",
    "        t_long_short['momr']='long_short'\n",
    "        \n",
    "        t_output =pd.concat([t_winners, t_losers, t_long_short])\\\n",
    "            .rename(columns={0:f'{K}_t-stat', 1:f'{K}_p-value'})\n",
    "        \n",
    "        # Combine mean, t and p\n",
    "        mom_output = pd.merge(mom_mean, t_output, on=['momr'], how='inner')\n",
    "        mom_output['momr'] = [f'{J} Sell', f'{J} Buy', f'{J} Buy-sell']\n",
    "        \n",
    "        mom_output = mom_output[['momr',f'{K}_mean',f'{K}_t-stat']]\n",
    "        Panel_B[(J,K)] = mom_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9e5f6d1f-61fa-4255-9dbd-d277970bc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_results1 = pd.concat([Panel_B[(3,3)],Panel_B[(6,3)],Panel_B[(9,3)],Panel_B[(12,3)]],ignore_index=True)\n",
    "skip_results2 = pd.concat([Panel_B[(3,6)],Panel_B[(6,6)],Panel_B[(9,6)],Panel_B[(12,6)]],ignore_index=True)     \n",
    "skip_results3 = pd.concat([Panel_B[(3,9)],Panel_B[(6,9)],Panel_B[(9,9)],Panel_B[(12,9)]],ignore_index=True) \n",
    "skip_results4 = pd.concat([Panel_B[(3,12)],Panel_B[(6,12)],Panel_B[(9,12)],Panel_B[(12,12)]],ignore_index=True) \n",
    "skip_result = pd.merge(skip_results1, skip_results2, on=['momr'], how='inner')\n",
    "skip_result = pd.merge(skip_result, skip_results3, on=['momr'], how='inner')\n",
    "skip_result = pd.merge(skip_result, skip_results4, on=['momr'], how='inner')\n",
    "skip_result.rename(columns={'momr':'J'},inplace=True)\n",
    "skip_result = result[['J','3_mean','3_t-stat','6_mean','6_t-stat','9_mean','9_t-stat','12_mean','12_t-stat']].round(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "25030bc5-5b62-4968-8b89-1f318d3cecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_result.to_csv('Table 2 Panel_B.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a1c2ab-839a-4d9d-967b-759eceb73fde",
   "metadata": {},
   "source": [
    "## Performance of portfolios double-sorted on recent and intermediate momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b8b79-b7ef-45b9-b2c2-69cdc7488cf1",
   "metadata": {},
   "source": [
    "In this part, we try to replicate Panel A of Table 4 of Novy-Marx (2012)which reports the excess returns(percent per month) of portfolios double-sorted by recent momentum and intermediate momentumwe. To maintain highly diversified portfolios weemploy quintile sorts (again using NYSE breakpoints) and portfolio returns are value-weighted. We use the risk-free rate as the benchmark.Sample period is from January 1927 to December 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "1c08011e-99f7-4e79-85df-e63f5fc37591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your WRDS username [benben]: lexuan\n",
      "Enter your password: ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Create .pgpass file now [y/n]?:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created .pgpass file successfully.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "conn = wrds.Connection()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "35604898-f088-48db-b417-8a02324ba8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = conn.raw_sql(\"\"\"  \n",
    "    SELECT table_name   \n",
    "    FROM information_schema.tables   \n",
    "    WHERE table_schema = 'ff';  \n",
    "\"\"\")  \n",
    "\n",
    "RF = conn.raw_sql(\"\"\"  \n",
    "                SELECT dateff, rf            \n",
    "                FROM \"ff\".\"factors_monthly\" \n",
    "                WHERE date BETWEEN '1927-01-01' AND '2023-12-31'  \n",
    "                \"\"\")  \n",
    "\n",
    "RF['dateff'] = pd.to_datetime(RF['dateff']) \n",
    "RF['hdate'] = RF['dateff'] + MonthEnd(0)\n",
    "RF = RF[['hdate','rf']]\n",
    "RF['hdate'] = pd.to_datetime(RF['hdate']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "b481ccf2-39c8-43fe-968f-58d8998d7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crsp_m = conn.raw_sql(\"\"\"\n",
    "                      select a.permno, a.date,\n",
    "                      b.shrcd, b.exchcd, \n",
    "                      a.ret, a.vol, a.shrout, a.prc\n",
    "                      from crsp.msf as a\n",
    "                      left join crsp.msenames as b\n",
    "                      on a.permno=b.permno\n",
    "                      and b.namedt<=a.date\n",
    "                      and a.date<=b.nameendt\n",
    "                      where a.date between '01/01/1927' and '12/31/2023'\n",
    "                      and b.exchcd between -2 and 2\n",
    "                      and b.shrcd between 10 and 11\n",
    "                      \"\"\") \n",
    "\n",
    "# Change variable format to int\n",
    "crsp_m[['permno','shrcd','exchcd']]=\\\n",
    "    crsp_m[['permno','shrcd','exchcd']].astype(int)\n",
    "\n",
    "# Line up date to be end of month\n",
    "crsp_m['date']=pd.to_datetime(crsp_m['date'])\n",
    "\n",
    "_tmp_crsp = crsp_m[['permno','date','ret']].sort_values(['permno','date'])\\\n",
    "    .set_index('date')\n",
    "_tmp_crsp['ret']=_tmp_crsp['ret'].fillna(0)\n",
    "_tmp_crsp['logret']=np.log(1+_tmp_crsp['ret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "698539b1-dd18-4621-bfc8-72456620baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_crsp['logret']=np.log(1+_tmp_crsp['ret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "3cfae379-e407-42ae-afc0-5b8fd083aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "umd_6 = _tmp_crsp.groupby('permno')['logret'].rolling(6, min_periods=6).sum().reset_index()\n",
    "umd_12 = _tmp_crsp.groupby('permno')['logret'].rolling(12, min_periods=12).sum().reset_index()\n",
    "umd_6['cumret_6']=np.exp(umd_6['logret'])-1\n",
    "umd_12['cumret_12']=np.exp(umd_12['logret'])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "f1d2eb33-d96a-42c3-bd1b-2a3310c238a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "umd = pd.merge(umd_6,umd_12,on=['permno','date'],how='inner')\n",
    "umd['mid_ret'] = umd['cumret_12']-umd['cumret_6']\n",
    "umd['recent_ret'] = umd['cumret_6']\n",
    "umd=umd.dropna(axis=0, subset=['mid_ret','recent_ret']).reset_index()\n",
    "umd = umd[['permno','date','mid_ret','recent_ret']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "e42d5e47-9cca-45f8-8d59-d0f01ad5bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each date: assign ranking 1-5 based on intermmediate and most recent 6-month cumulative return respectively\n",
    "# 1=lowest 5=highest cumret\n",
    "\n",
    "umd['mid_momr']=umd.groupby('date')['mid_ret'].transform(lambda x: pd.qcut(x, 5, labels=False))\n",
    "umd['recent_momr']=umd.groupby('date')['recent_ret'].transform(lambda x: pd.qcut(x, 5, labels=False))\n",
    "\n",
    "umd.mid_momr=umd.mid_momr.astype(int) \n",
    "umd['mid_momr'] = umd['mid_momr']+1\n",
    "umd.recent_momr=umd.recent_momr.astype(int) \n",
    "umd['recent_momr'] = umd['recent_momr']+1\n",
    "umd['form_date'] = umd['date']+ MonthEnd(0)\n",
    "umd['hdate']=umd['date']+MonthEnd(1)\n",
    "umd = umd[['permno','date','mid_momr','recent_momr','form_date','hdate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "057faa80-be33-4523-8707-e9b94a147b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_ret = crsp_m[['permno','date','ret','shrout','prc']]\n",
    "_tmp_ret = _tmp_ret[(_tmp_ret['prc'].notna()) & (_tmp_ret['shrout'].notna()) & (_tmp_ret['ret'].notna())]\n",
    "_tmp_ret['hdate']=  _tmp_ret['date']+ MonthEnd(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "de3368fb-c9f6-44f8-9768-6bcfbe36890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join rank and other necessary data together\n",
    "# Merge on 'hdate' i.e. the holding period\n",
    "\n",
    "_tmp_ret = crsp_m[['permno','date','ret','shrout','prc']]\n",
    "_tmp_ret = _tmp_ret[(_tmp_ret['prc'].notna()) & (_tmp_ret['shrout'].notna()) & (_tmp_ret['ret'].notna())]\n",
    "_tmp_ret['hdate']=  _tmp_ret['date']+MonthEnd(0)\n",
    "\n",
    "port = pd.DataFrame()\n",
    "n = 100000\n",
    "\n",
    "for i in range(0, _tmp_ret.shape[0], n): \n",
    "    chunks =   _tmp_ret.iloc[i:i + n]\n",
    "    merged = umd.merge(chunks, on=['permno','hdate'], how='left')\n",
    "    port = pd.concat([port, merged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "f803a49d-a6b4-42f9-92f1-06ed1a1bd144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate value weighted return\n",
    "def wavg(group, avg_name, weight_name):\n",
    "    d = group[avg_name]\n",
    "    w = group[weight_name]\n",
    "    try:\n",
    "        return (d * w).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return np.nan\n",
    "\n",
    "# value-weigthed return\n",
    "port['size'] = port['prc']*port['shrout'].abs()\n",
    "port_whgtret = port.groupby(['hdate', 'mid_momr','recent_momr']).apply(wavg, 'ret', 'size', include_groups=False).reset_index(name='vwret')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "43001bfc-a903-47f3-bfbd-1308f3f4a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ER = port_whgtret.merge(RF, on=['hdate'], how='left')\n",
    "ER['excuss_ret'] = ER['vwret']-ER['rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "d581f0f9-1b5d-4f47-b150-d97496e4e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Long_short portfolios in  Recent_momr given Mid_momr\n",
    "Mid_Portfolio = ER.pivot(index=['mid_momr','hdate'], columns= 'recent_momr', values= 'excuss_ret')\n",
    "Mid_Portfolio['5-1'] = Mid_Portfolio[5]-Mid_Portfolio[1] \n",
    "Mid_Portfolio.dropna(subset=['5-1'], inplace=True)  \n",
    "Summary_mid = Mid_Portfolio.groupby(level='mid_momr').describe()['5-1'].loc[:, ['mean']]\n",
    "t_values = Mid_Portfolio.groupby(level='mid_momr')['5-1'].apply(lambda x: stats.ttest_1samp(x, 0).statistic)      \n",
    "Summary_mid['t-value'] = t_values  \n",
    "\n",
    "#Get Long_short portfolios in  Mid_momr given Recent_momr\n",
    "Recent_Portfolio = ER.pivot(index=['recent_momr','hdate'], columns= 'mid_momr', values= 'excuss_ret')\n",
    "Recent_Portfolio['5-1'] = Recent_Portfolio[5]-Recent_Portfolio[1]\n",
    "Recent_Portfolio.dropna(subset=['5-1'], inplace=True)  \n",
    "Summary_recent = Recent_Portfolio.groupby(level='recent_momr').describe()['5-1'] .loc[:, ['mean']] #mid_momr 做差\n",
    "t_values = Recent_Portfolio.groupby(level='recent_momr')['5-1'].apply(lambda x: stats.ttest_1samp(x, 0).statistic)   \n",
    "Summary_recent['t-value'] = t_values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "ec80e86d-7567-4376-8912-eaf27f08b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_Output = ER.pivot_table(index='recent_momr', columns='mid_momr', values='vwret', aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "74f1910e-f4f1-44d7-88a0-0d67b94ceb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary_T = ER.groupby(['mid_momr', 'recent_momr'])['vwret'].apply(lambda x: stats.ttest_1samp(x, 0).statistic)  \n",
    "Summary_t_value_df = Summary_T.reset_index(name='t-statistic')  \n",
    "T_Output = Summary_t_value_df.pivot(index='recent_momr', columns='mid_momr', values='t-statistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "2f02f3e1-8e10-4c55-bf7e-aee637a3a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_Output['5-1'] = Summary_recent['mean']\n",
    "result_mean = pd.concat([Mean_Output,Summary_mid['mean'].T.to_frame().T],axis=0)\n",
    "result_mean.columns = ['IR1','IR2','IR3','IR4','IR5','long-short']\n",
    "new_index = ['RR1','RR2','RR3','RR4','RR5','long-short']\n",
    "result_mean.index = new_index \n",
    "result_mean = (result_mean*100).round(2)\n",
    "result_mean.name = \"Estimate (percent)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "01a499a2-5dfa-4ed4-8f87-1fb2510e7a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T_Output['5-1'] = Summary_recent['t-value']\n",
    "result_t = pd.concat([T_Output, Summary_mid['t-value'].T.to_frame().T], axis=0)\n",
    "result_t.columns = ['IR1','IR2','IR3','IR4','IR5','long-short']\n",
    "result_t.index = new_index \n",
    "result_t = result_t.round(2)\n",
    "result_t.name = \"Test statistics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "60db8442-43c9-4b9c-b8a0-2f76a90a2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.concat([result_mean, result_t], axis=1) \n",
    "final_result.columns = pd.MultiIndex.from_tuples(  \n",
    "    [(result_mean.name, col) for col in result_mean.columns] +  \n",
    "    [(result_t.name, col) for col in result_t.columns])    \n",
    "final_result.index.name = None \n",
    "final_result.to_csv('Table 4 Panel_A.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e46b91-c414-47f0-89e9-088be76c7223",
   "metadata": {},
   "source": [
    "## Brief Analysis and Discussion\n",
    "Our replicate result reinforces the previously established conclusions, demonstrating that both intermediate horizon performance and recent past performance are significantly correlated with future returns. However, recent past performance exhibits a lesser predictive capability.\n",
    "\n",
    "The analysis reveals that while return spreads are generally significant, the magnitude and significance of the differences between intermediate horizon winners and losers are notable. For instance, the long-short estimates indicate a positive return for intermediate horizon winners compared to losers, particularly in IR5, where the estimate is 2.61% for winners and 6.19% for losers, suggesting a significant spread.\n",
    "\n",
    "A direct comparison between recent winners who were intermediate horizon losers and recent losers who had also been intermediate horizon losers reveals interesting dynamics. Specifically, stocks that have significantly increased in value over the past six months (e.g., RR5 in IR1 with a return of 3.07%) but performed poorly in the prior six months tend to underperform relative to stocks that have decreased in value but performed well earlier (e.g., RR4 in IR4 with a return of 1.83%).\n",
    "\n",
    "The monthly return differential between the intermediate horizon winners – recent losers portfolio and the recent winners – intermediate horizon losers portfolio is recorded at 0.95% (long-short estimate for RR1), with a test statistic of 0.40. This indicates a weaker predictive power for recent past performance compared to intermediate horizon performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PycharmEnv",
   "language": "python",
   "name": "pycharmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
